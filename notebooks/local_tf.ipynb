{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\nUsing default strategy for CPU and single GPU\nREPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "VERBOSE = 2\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "IMAGE_SIZE = (1024,1024)\n",
    "IMAGE_SHAPE = (1024,1024,3)\n",
    "\n",
    "strategy = tf.distribute.get_strategy() \n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Using default strategy for CPU and single GPU\")\n",
    "print(\"REPLICAS:\", REPLICAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensorflow version 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "cwd = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train TFRecord Files: 10\nValidation TFRecord Files: 5\nTest TFRecord Files: 16\n"
     ]
    }
   ],
   "source": [
    "TRAINING_FILENAMES = tf.io.gfile.glob(cwd + \"/data/train*.tfrec\")[:10]\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(cwd + \"/data/train*.tfrec\")[10:16]\n",
    "TEST_FILENAMES = tf.io.gfile.glob(cwd + \"/data/test*.tfrec\")\n",
    "\n",
    "print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\n",
    "print(\"Validation TFRecord Files:\", len(VALIDATION_FILENAMES))\n",
    "print(\"Test TFRecord Files:\", len(TEST_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "  features = tf.io.parse_example(\n",
    "      serialized_example,\n",
    "      # Defaults are not specified since both keys are required.\n",
    "      features={\n",
    "          'image': tf.io.FixedLenFeature([], tf.string),\n",
    "          'target': tf.io.FixedLenFeature([], tf.int64),\n",
    "      })\n",
    "\n",
    "  # Convert from a scalar string tensor (whose single string has\n",
    "  # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n",
    "  # [mnist.IMAGE_PIXELS].\n",
    "  image = tf.io.decode_raw(features['image'], tf.uint8)\n",
    "  #image.set_shape((mnist.IMAGE_PIXELS))\n",
    "\n",
    "  # Convert target from a scalar uint8 tensor to an int32 scalar.\n",
    "  target = tf.cast(features['target'], tf.int32)\n",
    "  \n",
    "  return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "  # OPTIONAL: Could reshape into a 28Ã—28 image and apply distortions\n",
    "  # here.  Since we are not applying any distortions in this\n",
    "  # example, and the next step expects the image to be flattened\n",
    "  # into a vector, we don't bother.\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(is_train, batch_size = 32, num_epochs = 40):\n",
    "    \"\"\"Get a TF-Dataset.\n",
    "\n",
    "    Args:\n",
    "    is_train: Selects between the training (True) and validation (False) data.\n",
    "    batch_size: Number of examples per returned batch.\n",
    "    num_epochs: Number of times to read the input data\n",
    "\n",
    "    Returns:\n",
    "    A tuple (images, labels), where:\n",
    "    * images is a float tensor with shape [batch_size, mnist.IMAGE_PIXELS]\n",
    "        in the range [-0.5, 0.5].\n",
    "    * labels is an int32 tensor with shape [batch_size] with the true label,\n",
    "        a number in the range [0, mnist.NUM_CLASSES).\n",
    "    This function creates a one_shot_iterator, meaning that it will only iterate\n",
    "    over the dataset once. On the other hand there is no special initialization\n",
    "    required.\n",
    "    \"\"\"\n",
    "\n",
    "    tf_records = TRAINING_FILENAMES if is_train else VALIDATION_FILENAMES\n",
    "    print(\"Reading file containing\", count_data_items(tf_records), \"items.\")\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False #disabling order, increasing speed\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        # TFRecordDataset opens a protobuf and reads entries line by line\n",
    "        # could also be [list, of, filenames]\n",
    "        dataset = tf.data.TFRecordDataset(tf_records, num_parallel_reads=AUTOTUNE)\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "        # the parameter is the queue size\n",
    "        dataset = dataset.shuffle(1000 + 3 * batch_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        # map takes a python function and applies it to every sample\n",
    "        dataset = dataset.map(decode)\n",
    "        dataset = dataset.map(augment)\n",
    "        dataset = dataset.map(normalize)\n",
    "\n",
    "        dataset = dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "        #iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    #return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=BATCH_SIZE):\n",
    "    lr_start   = 0.000005\n",
    "    lr_max     = 0.00000125 * REPLICAS * batch_size\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading file containing 21788 items.\n",
      "Reading file containing 10904 items.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1024, 1024, 3)]   0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b6 (Functional) (None, 32, 32, 2304)      40960136  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2305      \n",
      "=================================================================\n",
      "Total params: 40,962,441\n",
      "Trainable params: 40,738,009\n",
      "Non-trainable params: 224,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    is_train=True\n",
    "    train_ds = get_dataset(is_train)\n",
    "\n",
    "    is_train=False\n",
    "    val_ds = get_dataset(is_train)\n",
    "\n",
    "    files_train = count_data_items(TRAINING_FILENAMES)\n",
    "    steps_per_epoch=files_train/BATCH_SIZE//REPLICAS\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=IMAGE_SHAPE)\n",
    "    base = efn.EfficientNetB6(input_shape=IMAGE_SHAPE,weights='imagenet',include_top=False)\n",
    "\n",
    "    x = base(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp,outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n",
    "\n",
    "    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=EPOCHS, \n",
    "        callbacks = [get_lr_callback()], \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_ds,\n",
    "        verbose=VERBOSE\n",
    "    )"
   ]
  }
 ]
}